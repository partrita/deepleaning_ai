{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://learn.deeplearning.ai/courses/pretraining-llms/lesson/xg5n5/why-pre-training\n",
    "\n",
    "> AI is the new electricity and will transform and improve nearly all areas of human lives. [Andrew Ng](https://www.andrewng.org/) 가장 마지막으로 갈 것.\n",
    "\n",
    "# Pre-training LLM\n",
    "\n",
    "- sung kim and lucy park, Pre-training LLM, deeplearning.ai\n",
    "\n",
    "Training a large language model (LLM) from scratch is computationally expensive and time-consuming. Pre-training a large language model on a large corpus of text data and then fine-tuning it on a smaller dataset for a specific task is a common practice in natural language processing (NLP). In this article, we will discuss the benefits of pre-training LLMs and how to pre-train them.\n",
    "\n",
    "## Purpose of pre-training LLM\n",
    "\n",
    "Some are buiding models for tasks in specific domains like legal, healthcare and e-commmerse. other in specific languages like Chinese, Arabic and Korean. Furthermore, new training methods are making it more efficient pre-training possible like depth upscaling, which uses two or more sets of exising models to build larger models. \n",
    "\n",
    "In this course, you'll learn all of the necessary steps to pre-train a model from scratch, from gathering and preparing train data, to configuring a model and training it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo:false\n",
    "\n",
    "# Ignore insignificant warnings (ex: deprecations)\n",
    "import warnings\n",
    "# Set a seed for reproducibility\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a general pretrained model\n",
    "\n",
    "This course will work with small models that fit within the memory of the learning platform. TinySolar-248m-4k is a small decoder-only model with 248M parameters (similar in scale to GPT2) and a 4096 token context window. You can find the model on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k).\n",
    "\n",
    "You'll load the model in three steps:\n",
    "1. Specify the path to the model in the Hugging Face model library\n",
    "2. Load the model using `AutoModelforCausalLM` in the `transformers` library\n",
    "3. Load the tokenizer for the model from the same model path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Data Preparation\n",
    "\n",
    "When pre-training a language model, the quality of the data you use is crucial. In this lesson you'll carry out some of the data cleaning steps required to prepare data for pretraining. In the video, Sung mentioned an Upstage tool called **Dataverse** which can help you with data cleaning. You can checkout the features of Dataverse at [this link](https://github.com/UpstageAI/dataverse).\n",
    "\n",
    "**Sourcing datasets for pretraining**\n",
    "\n",
    "In this section, you'll see two ways to source data for training:\n",
    "1. Download an existing dataset from Hugging Face\n",
    "2. Create a dataset of python scripts sourced from Github\n",
    "\n",
    "In both cases the result will be a Hugging Face `Dataset` object, part of the `Datasets` library. You can read more about the properties of Datasets and how to work with them on the [Hugging Face website](https://huggingface.co/docs/datasets/en/index).\n",
    "\n",
    "## Download data from Hugging face\n",
    "\n",
    "The dataset you download here is a subset of a much larger dataset called **Red Pajama**. The full, 1 trillion token dataset is available on Hugging Face at [this link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 60000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"upstage/Pretraining_Dataset\", split=\"train\"\n",
    ")\n",
    "pretraining_dataset = pretraining_dataset.select_columns([\"text\"])\n",
    "print(pretraining_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare pretraining and fine-tuning datasets\n",
    "\n",
    "In the next cell, you'll download a fine-tuning dataset to contrast with the pretraining dataset you loaded above. You can read more about the Alpaca model and instruction tuning dataset [here](https://crfm.stanford.edu/2023/03/13/alpaca.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Input: \n",
      "Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset = datasets.load_dataset(\"c-s-ale/alpaca-gpt4-data\", split=\"train\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "print(\n",
    "    \"Instruction: \"\n",
    "    + instruction_dataset[i][\"instruction\"]\n",
    "    + \"\\nInput: \"\n",
    "    + instruction_dataset[i][\"input\"]\n",
    "    + \"\\nOutput: \"\n",
    "    + instruction_dataset[i][\"output\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in contrast to the pretraining data, which is just raw text, fine-tuning datasets are structured into question-answer pairs or instruction-response sets that can include additional input context if required. \n",
    "\n",
    "\n",
    "Moving forward, you'll only work with the unstructured pretraining dataset.\n",
    "\n",
    "## Scrape python code from Github\n",
    "\n",
    "Here, you'll download a selection of python scripts from Github and then prepare them as a Hugging Face `Dataset` object to use in training. \n",
    "\n",
    "The same pattern here will work for preparing any text scraped from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_util.py\n",
      "240628_learn_async.py\n",
      "double_linear_search_recursion.py\n",
      "values.py\n",
      "test_subgraph_rewriter.py\n",
      "version.py\n",
      "llama_cpp_script.py\n",
      "huggingface_test.py\n",
      "240628_gen_mutants.py\n",
      "numpy_mlp.py\n",
      "230413_Python_Tish2Downloader.py\n",
      "visualize.py\n",
      "240628_pdb-seq.py\n",
      "distribute_coordinator_context.py\n",
      "__init__.py\n",
      "dotenv.py\n",
      "finetune_tutorial.py\n",
      "linear_reg.py\n",
      "KULLM.py\n"
     ]
    }
   ],
   "source": [
    "# Import some required packages\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Path to directory to store python scripts\n",
    "code_dir = \"../src/\"\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\",\n",
    "    \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\",\n",
    "    \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\",\n",
    "    \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\",\n",
    "    \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\",\n",
    "    \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\",\n",
    "    \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\",\n",
    "    \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\",\n",
    "    \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\",\n",
    "]\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_path = os.path.join(code_dir, file_name)\n",
    "\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "files = os.listdir(code_dir)\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 60019\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dataset = []\n",
    "\n",
    "for file in os.listdir(code_dir):\n",
    "    code_dataset.append({\"text\": open(os.path.join(code_dir, file), \"r\").read()})\n",
    "\n",
    "code_dataset = datasets.Dataset.from_list(code_dataset)\n",
    "\n",
    "dataset = datasets.concatenate_datasets([pretraining_dataset, code_dataset])\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In the cells below, you'll carry out the following cleaning steps:\n",
    "1. Filter out samples that are too short\n",
    "2. Remove repetitions within a single text example\n",
    "3. Remove duplicated documents\n",
    "4. Quality filter to remove non-English texts\n",
    "\n",
    "### Remove examples that are too short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63afe6a8e4164d3893912dfa5c8dfdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/60019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "import re\n",
    "\n",
    "\n",
    "def paragraph_length_filter(x):\n",
    "    \"\"\"Returns False iff a page has too few lines or lines are too short.\"\"\"\n",
    "    lines = x[\"text\"].split(\"\\n\")\n",
    "    if len(lines) < 3 or min(heapq.nlargest(3, [len(line) for line in lines])) < 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "dataset = dataset.filter(paragraph_length_filter, load_from_cache_file=False)\n",
    "dataset.num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove repeated text within training examples\n",
    "\n",
    "Here you'll remove text repetitions within each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a3aade883f4508b2074c0e2d98d103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52336"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_duplicates(paragraphs):\n",
    "    \"\"\"\n",
    "    Use this function to find the number of repetitions\n",
    "    in the paragraphs.\n",
    "    \"\"\"\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in paragraphs:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars\n",
    "\n",
    "def paragraph_repetition_filter(x):\n",
    "    \"\"\"\n",
    "    Returns False iff a page has too many repetitions.\n",
    "    \"\"\"\n",
    "    text = x[\"text\"]\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(\n",
    "        text.strip()\n",
    "    )  # Split by paragraphs (2 or more newlines)\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(\n",
    "        paragraphs\n",
    "    )  # Find number of duplicates in paragraphs\n",
    "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if char_duplicates / len(text) > 0.2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dataset = dataset.filter(paragraph_repetition_filter, load_from_cache_file=False)\n",
    "dataset.num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "In this section, you'll remove duplicate examples from the entire dataset (in contrast to the previous step where you were just looking for repeated text in each example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc5e88f7c694dae9ee928da1338b3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "43607"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deduplication(ds):\n",
    "    def dedup_func(x):\n",
    "        \"\"\"Use this function to remove duplicate entries\"\"\"\n",
    "        if x[\"text\"] in unique_text:\n",
    "            return False\n",
    "        else:\n",
    "            unique_text.add(x[\"text\"])\n",
    "            return True\n",
    "\n",
    "    unique_text = set()\n",
    "\n",
    "    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = deduplication(dataset)\n",
    "dataset.num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality filter - Language\n",
    "\n",
    "Here you'll remove any text examples that are in a language other than English. The code here uses a language detection model called fastText. You can read about fastText [here](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function english_language_filter.<locals>.is_english at 0x7d5067fdf880> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9173acfa64054d83acd0fe14392fe592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/43607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40476"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from fasttext.FastText import _FastText\n",
    "\n",
    "\n",
    "def english_language_filter(ds):\n",
    "    # load language detection model\n",
    "    model = _FastText(\"../models/L2_language_model.bin\")\n",
    "\n",
    "    def is_english(x):\n",
    "        # Predict language of the text and probability\n",
    "        language, score = model.predict(x[\"text\"].replace(\"\\n\", \"\"))\n",
    "\n",
    "        # Check if the language is English and the score is above 0.4\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        return (\n",
    "            score > 0.4 and language == \"en\"\n",
    "        )  # change code here if building a model in another language\n",
    "\n",
    "    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = english_language_filter(dataset)\n",
    "dataset.num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset to disk\n",
    "\n",
    "Read more about the parquet data format [here](https://parquet.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2851e74012f0431bb9977ef8e3daaa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "197102794"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../data/output/preprocessed_dataset.parquet\"\n",
    "dataset.to_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 40476\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned your datasets, you need to prepare it for training. In this section, you'll learn how to packages your training data so that it can be used in HugginfFace\n",
    "\n",
    "\n",
    "\n",
    "# Lesson 3: Data Packaging\n",
    "\n",
    "Data packaging tokeninzing + packing\n",
    "\n",
    "Tokenizing breaking each text into smaller, meaningful units, which are callked token. Packing tokens into the maximum sequence length to improve training efficiency.\n",
    "\n",
    "## 1. Tokenizing and creating input_ids\n",
    "\n",
    "Start by loading the dataset from the previous lesson:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6cc113cb504923b43df3bf17410aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 40476\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"parquet\", data_files=\"../data/output/preprocessed_dataset.parquet\", split=\"train\"\n",
    ")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `shard` method of the Hugging Face `Dataset` object to split the dataset into 10 smaller pieces, or *shards* (think shards of broken glass). You can read more about sharding at [this link](https://huggingface.co/docs/datasets/en/process#shard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shard(num_shards=10, index=0)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path_or_name = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    use_fast=False # Note: in this case, long text samples sometimes tend to hang, so we disable fast tokenization. Instead use the map function and the datasets library for parallel processing.\n",
    "    )\n",
    "tokenizer.tokenize(\"I'm a short sentence\")\n",
    "\n",
    "# Create a helper function:\n",
    "def tokenization(example):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "\n",
    "    # Convert tokens to ids\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Add <bos>, <eos> tokens to the front and back of tokens_ids\n",
    "    # bos: begin of sequence, eos: end of sequence\n",
    "    token_ids = [tokenizer.bos_token_id] + token_ids + [tokenizer.eos_token_id]\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # We will be using this column to count the total number of tokens\n",
    "    # in the final dataset\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize all the examples in the pretraining dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7b434180f14cc0a7191235cbd138ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'num_tokens'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text The Colorado Climate Center pr\n",
      "\n",
      "input_ids [1, 415, 15837, 1366, 3314, 6064, 5312, 430, 19102, 304, 1178, 356, 281, 3928, 28725, 9735, 28713, 28725, 264, 1052, 14455, 4623, 28725, 9390, 1452, 274, 28725, 17268, 28713, 28725]\n",
      "\n",
      "num_tokens 549\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[3]\n",
    "\n",
    "print(\"text\", sample[\"text\"][:30])  #\n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total number of tokens in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113663"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sum(dataset[\"num_tokens\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Packing the data\n",
    "\n",
    "![Packing data for training](./data_packing.png)\n",
    "\n",
    "Concatenate input_ids for all examples into a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5113663\n",
      "5113632\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.concatenate(dataset[\"input_ids\"])\n",
    "print(len(input_ids))\n",
    "max_seq_length = 32\n",
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(total_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard extra tokens from end of the list so number of tokens is exactly divisible by `max_seq_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5113632,)\n",
      "(159801, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids[:total_length]\n",
    "print(input_ids.shape)\n",
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "print(input_ids_reshaped.shape)\n",
    "print(type(input_ids_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Hugging Face dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 159801\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict({\"input_ids\": input_ids_list})\n",
    "print(packaged_pretrain_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save the packed dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de771bcbe1594ebba245035579e98953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21093732"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packaged_pretrain_dataset.to_parquet(\"../data/output/packaged_pretrain_dataset.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 159801\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packaged_pretrain_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 560, 28705, 28740, 28787, 28774, 28770, 1054, 14886, 23452]\n"
     ]
    }
   ],
   "source": [
    "print(packaged_pretrain_dataset[0][\"input_ids\"][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Preparing your model for training\n",
    "\n",
    "## 1. Model configuration\n",
    "\n",
    "You'll configure models based on Meta's Llama family of models. The transformers library has several tools for working with these models, which you can read about [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama).\n",
    "\n",
    "Start by creating a `LlamaConfig` object to configure the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")\n",
    "\n",
    "config = LlamaConfig()\n",
    "print(config)\n",
    "\n",
    "# Next, update parameters to change the model architecture:\n",
    "config.num_hidden_layers = 12  # reduced from 32 to 12\n",
    "config.hidden_size = 1024  # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = (\n",
    "    4096  # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    ")\n",
    "config.num_key_value_heads = (\n",
    "    8  # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    ")\n",
    "config.torch_dtype = \"bfloat16\"  # for half-precision training\n",
    "config.use_cache = False  # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight initialization\n",
    "\n",
    "In the next sections, you'll explore four different ways to initialize the weights of a model for training:\n",
    "1. Random weight initialization\n",
    "2. Using an existing model for continued pre-training\n",
    "3. Downscaling an existing model\n",
    "4. Upscaling an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.num_hidden_layers = 12      # reduced from 32 to 12\n",
    "config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"    # for half-precision training\n",
    "config.use_cache = False           # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random weight initialization\n",
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation of 0.02. Values beyond 2-sigma from the mean are set to 0.\n",
    "\n",
    "```python\n",
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM(config)\n",
    "print_nparams(model)  # 248013824 => 248M\n",
    "\n",
    "# Take a look at a sample of the weights in a single layer:\n",
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}':\")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break\n",
    "# Try using the model for inference:\n",
    "# Load a tokenizer from Upstage Solar, \n",
    "# which is compatible with the Llama-2 tokenizer\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "model_dir = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Remove the model from memory to avoid crashing the kernel:\n",
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()\n",
    "```\n",
    "\n",
    "### Reuse general pretrained model weights\n",
    "\n",
    "If you load an existing model, you can use it as is to continue pretraining on new data.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "```\n",
    "\n",
    "### Downscaling from a general pretrained model\n",
    "\n",
    "Here you'll downscale the tinySolar-248m-4k model from a 12 layer model to a 10 layer model.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "layers = model.model.layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,    \n",
    "    num_hidden_layers=len(model.model.layers),\n",
    ")\n",
    "model.config = config\n",
    "\n",
    "print_nparams(model)  # 217601024 => 217M\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Depth Upscaling from a general pretrained model\n",
    "\n",
    "Here you are going to upscale the `tinySolar-248m-4k` model from 12 layers to 16 layers. Here are the steps you'll take:\n",
    "\n",
    "1. Configure a 16 layer model and initialize it with random weights\n",
    "2. Load the 12 layer `tinySolar-248m-4k` model into memory\n",
    "3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "The total number of parameters is: 308839424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bc9d26626f4bbba9db21dc67ed5958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1088a3b135a4d6e825037d1de6aa21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e4139b14634d90bb6b3f87db683700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfef79560e1f4083b4ad27af8fd86fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fda5a9ffa345918665b07d0dc8aeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabb9dd2ca8b40798600d0e705e36c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca78354861b4acb94b2a71748012602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 248013824\n",
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "to work with people who are not afraid to look at the world and are not afraid to look at the world with a little bit of a twist.\n",
      "I am a very humble person and I am very fortunate to have a great team of people who work hard to make a difference.\n",
      "I am very fortunate to have a great team of people who work hard to make a difference.\n",
      "I am very fortunate to have a great team of people who work hard to make a difference.\n",
      "I am very fortunate to have a great team of people who work hard to make a difference.\n",
      "I am very fortunate to have a great team\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "\n",
    "config = LlamaConfig(\n",
    "    num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False\n",
    ")\n",
    "print(config)\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(dtype=torch.bfloat16)  # convert to bfloat16\n",
    "print_nparams(model)  # 308839424 => 308M\n",
    "\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(pretrained_model) #  248013824 => 248M\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) \\\n",
    "    + deepcopy(pretrained_model.model.layers[4:])\n",
    "\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# Run simple inference to show no trained model\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model to disk\n",
    "Note the new model name here which reflects the 308 million parameters of the new, upscaled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../models/TinySolar-308m-4k-init')\n",
    "\n",
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5. Model training\n",
    "\n",
    "Pretraining is very expensive! Please check costs carefully before starting a pretraining project.\n",
    "\n",
    "You can get a rough estimate your training job cost using [this calculator](https://huggingface.co/training-cluster) from Hugging Face. For training on other infrastructure, e.g. AWS or Google Cloud, please consult those providers for up to date cost estimates. \n",
    "\n",
    "\n",
    "## 1. Load the model to be trained\n",
    "\n",
    "Load the upscaled model from the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../models/TinySolar-308m-4k-init\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")\n",
    "pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "\n",
    "Here you'll update two methods on the `Dataset` object to allow it to interface with the trainer. These will be applied when you specify the dataset you created in Lesson 3 as the training data in the next section.\n",
    "\n",
    "Note that the code has additional comment strings that don't appear in the video. These are to help you understand what each part of the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, split=\"train\"):\n",
    "        \"\"\"Initializes the custom dataset object.\"\"\"\n",
    "        self.args = args\n",
    "        self.dataset = datasets.load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=args.dataset_name,\n",
    "            split=split\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single data sample from the dataset\n",
    "        at the specified index\n",
    "        \"\"\"\n",
    "        # Convert the lists to a LongTensor for PyTorch\n",
    "        input_ids = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        labels = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "\n",
    "        # Return the sample as a dictionary\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training Arguments\n",
    "\n",
    "Here you set up the training run. The training dataset you created in Lesson 3 is specified in the Dataset configuration section.\n",
    "\n",
    "Note: there are comment strings in the cell below that don't appear in the video. These have been included to help you understand what each parameter does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import transformers\n",
    "\n",
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    dataset_name: str = field(                           # Dataset configuration\n",
    "        default=\"../data/output/packaged_pretrain_dataset.parquet\")\n",
    "    num_proc: int = field(default=1)                     # Number of subprocesses for data preprocessing\n",
    "    max_seq_length: int = field(default=32)              # Maximum sequence length\n",
    "\n",
    "    # Core training configurations\n",
    "    seed: int = field(default=0)                         # Random seed for initialization, ensuring reproducibility\n",
    "    optim: str = field(default=\"adamw_torch\")            # Optimizer, here it's AdamW implemented in PyTorch\n",
    "    max_steps: int = field(default=10000) # Number of maximum training steps\n",
    "    per_device_train_batch_size: int = field(default=2)  # Batch size per device during training\n",
    "\n",
    "    # Other training configurations\n",
    "    learning_rate: float = field(default=5e-5)           # Initial learning rate for the optimizer\n",
    "    weight_decay: float = field(default=0)               # Weight decay\n",
    "    warmup_steps: int = field(default=10)                # Number of steps for the learning rate warmup phase\n",
    "    lr_scheduler_type: str = field(default=\"linear\")     # Type of learning rate scheduler\n",
    "    gradient_checkpointing: bool = field(default=True)   # Enable gradient checkpointing to save memory\n",
    "    dataloader_num_workers: int = field(default=2)       # Number of subprocesses for data loading\n",
    "    bf16: bool = field(default=True)                     # Use bfloat16 precision for training on supported hardware\n",
    "    gradient_accumulation_steps: int = field(default=1)  # Number of steps to accumulate gradients before updating model weights\n",
    "\n",
    "    # Logging configuration\n",
    "    logging_steps: int = field(default=100)                # Frequency of logging training information\n",
    "    report_to: str = field(default=\"none\")               # Destination for logging (e.g., WandB, TensorBoard)\n",
    "\n",
    "    # Saving configuration\n",
    "    save_strategy: str = field(default=\"steps\")          # Can be replaced with \"epoch\"\n",
    "    save_steps: int = field(default=100)                   # Frequency of saving training checkpoint\n",
    "    save_total_limit: int = field(default=2)             # The total number of checkpoints to be saved\n",
    "\n",
    "# Parse the custom arguments and set the output directory where the model will be saved:\n",
    "\n",
    "parser = transformers.HfArgumentParser(CustomArguments)\n",
    "args, = parser.parse_args_into_dataclasses(\n",
    "    args=[\"--output_dir\", \"output\"]\n",
    ")\n",
    "\n",
    "# Setup the training dataset:\n",
    "train_dataset = CustomDataset(args=args)\n",
    "\n",
    "# Check the shape of the dataset:\n",
    "\n",
    "print(\"Input shape: \", train_dataset[0]['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the trainer and monitor the loss\n",
    "\n",
    "First, set up a callback to log the loss values during training (note this cell is not shown in the video):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "# Define a custom callback to log the loss values\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "\n",
    "# Initialize the callback\n",
    "loss_logging_callback = LossLoggingCallback() # 콜백 객체 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create an instance of the Hugging Face `Trainer` object from the `transformers` library. Call the `train()` method of the trainder to initialize the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 08:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>4.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>4.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>4.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>4.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>4.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>4.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>4.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>4.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>4.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>4.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>4.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>4.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>4.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>4.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>4.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>4.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>4.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>4.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>4.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>4.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>4.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>4.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>4.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>4.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>4.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>4.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>4.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.010400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=4.09177190246582, metrics={'train_runtime': 480.577, 'train_samples_per_second': 41.617, 'train_steps_per_second': 20.808, 'total_flos': 1060114268160000.0, 'train_loss': 4.09177190246582, 'epoch': 0.13})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    callbacks=[loss_logging_callback]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ggplot 스타일 적용\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "logs = loss_logging_callback.logs\n",
    "# loss_logging_callback에서 기록한 손실 값 가져오기\n",
    "losses = [log['loss'] for log in logs if 'loss' in log] # 'loss' 키가 있는 로그만 추출\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # 배경색 설정\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/TinySolar-308m-4k-finetune/tokenizer_config.json',\n",
       " '../models/TinySolar-308m-4k-finetune/special_tokens_map.json',\n",
       " '../models/TinySolar-308m-4k-finetune/tokenizer.model',\n",
       " '../models/TinySolar-308m-4k-finetune/added_tokens.json',\n",
       " '../models/TinySolar-308m-4k-finetune/tokenizer.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = '../models/TinySolar-308m-4k-finetune'\n",
    "\n",
    "pretrained_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance of an intermediate checkpoint\n",
    "\n",
    "Below, you can try generating text using an intermediate checkpoint of the model. This checkpoint was saved after 10,000 training steps. As you did in previous lessons, you'll use the Solar tokenizer and then set up a `TextStreater` object to display the text as it is generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what I see in her heart that it is her own. I'm a strong woman, who likes your side of everything and is so happy to give a great contribution to the people.\n",
      "If anyone has heard of your name, let him know you do indeed have a sister, but I'm not a member\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"./output/checkpoint-10000\"\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model2.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model2.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6. Model evaluation\n",
    "\n",
    "The model comparison tool that Sung described in the video can be found at this link: https://console.upstage.ai/ (note that you need to create a free account to try it out.)\n",
    "\n",
    "A useful tool for evaluating LLMs is the **LM Evaluation Harness** built by EleutherAI. Information about the harness can be found at this [github repo](https://github.com/EleutherAI/lm-evaluation-harness):\n",
    "\n",
    "You can run the commented code below to install the evaluation harness in your own environment:\n",
    "\n",
    "```bash\n",
    "#!pip install -U git+https://github.com/EleutherAI/lm-evaluation-harness\n",
    "```\n",
    "\n",
    "You will evaluate TinySolar-248m-4k on 5 questions from the **TruthfulQA MC2 task**. This is a multiple-choice question answering task that tests the model's ability to identify true statements. You can read more about the TruthfulQA benchmark in [this paper](https://arxiv.org/abs/2109.07958), and you can checkout the code for implementing the tasks at this [github repo](https://github.com/sylinrl/TruthfulQA).\n",
    "\n",
    "The code below runs only the TruthfulQA MC2 task using the LM Evaluation Harness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for the Hugging Face Leaderboard\n",
    "You can use the code below to test your own model against the evaluations required for the [Hugging Face leaderboard](https://huggingface.co/open-llm-leaderboard). \n",
    "\n",
    "If you decide to run this evaluation on your own model, don't change the few-shot numbers below - they are set by the rules of the leaderboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
